{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This is an implementation of the artistic neural network as described in this [paper](www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf). VGG pretrained model weights can be downloaded from [here](http://www.vlfeat.org/matconvnet/models/). Use imagenet-vgg-verydeep-19.mat and imagenet-vgg-verydeep-16.mat.  \n",
    "  \n",
    "Checklist for dependencies:\n",
    " - Tensorflow, preferably with GPU support\n",
    " - numpy\n",
    " - scipy\n",
    " - PIL\n",
    " - If you want to run in notebook, IPython notebook is also required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the pretrained weights\n",
    "The following bash code will automatically download the models. VGG19 is approximately 510M and VGG16 is approximately 491M. Comment out using \"#\" if you already have the model downloaded somewhere else. But don't forget to change the model path later (you will see it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!if [ ! -e \"models/imagenet-vgg-verydeep-19.mat\" ]; then echo \"VGG19 does not exist\"; wget -P models/ http://www.vlfeat.org/matconvnet/models/imagenet-vgg-verydeep-19.mat; fi\n",
    "!if [ ! -e \"models/imagenet-vgg-verydeep-16.mat\" ]; then echo \"VGG16 does not exist\"; wget -P models/ http://www.vlfeat.org/matconvnet/models/imagenet-vgg-verydeep-16.mat; fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "import scipy.io\n",
    "import PIL.Image\n",
    "import cPickle as pickle\n",
    "from IPython.display import clear_output, Image, display\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the base class for the VGG model\n",
    "This class will load the model accordingly, and wrap the pretrained weights with a Tensorflow graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class VGG(object):\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        self.content_layer = None\n",
    "        self.style_layer = None\n",
    "        self.path = path\n",
    "        self.weights = scipy.io.loadmat(self.path)['layers']\n",
    "    \n",
    "    def create_graph(self, input_shape):\n",
    "        \"\"\"\n",
    "        The pretrained model contains the layer name and layer type (i.e. pool, conv etc.)\n",
    "        To access those information, we can do the index access:\n",
    "        vgg_layers[0]      [0]       [0]      [0]      [2]                                    [0]      [0] ## weight\n",
    "        vgg_layers[0]      [0]       [0]      [0]      [2]                                    [0]      [1] ## bias\n",
    "                  always 0 |layer idx|always 0|always 0|0:layer name; 1:layer type; 2: weights|always 0|0:weight; 1:bias\n",
    "        vgg_layers[0][30][0][0][0][0] # to access layer name\n",
    "        vgg_layers[0][30][0][0][1][0] # to access layer type\n",
    "        \n",
    "        Note that the fully connected layers and the softmax are not required for this task, therefore we will skip it. \n",
    "        The fully connected layers have name fc* (It's type is conv though).\n",
    "        \"\"\"\n",
    "        if self.path == None:\n",
    "            raise Exception(\"Run from the child class\")\n",
    "            \n",
    "        vgg_layers = self.weights\n",
    "        num_layers = len(vgg_layers[0])\n",
    "        \n",
    "        graph = {}\n",
    "        layer_names = []\n",
    "        graph[\"input\"] = tf.Variable(np.zeros(input_shape), dtype=tf.float32)\n",
    "        prev = \"input\"\n",
    "        layer_names.append(\"input\")\n",
    "        \n",
    "        for idx in range(num_layers):\n",
    "            \n",
    "            layer_name = vgg_layers[0][idx][0][0][0][0]\n",
    "            layer_type = vgg_layers[0][idx][0][0][1][0]\n",
    "            \n",
    "            if layer_name[:2] == \"fc\":\n",
    "                break                     # stop before adding the first fc layer\n",
    "            \n",
    "            layer_names.append(layer_name)\n",
    "            \n",
    "            if layer_type == \"conv\":\n",
    "                W = vgg_layers[0][idx][0][0][2][0][0]\n",
    "                b = vgg_layers[0][idx][0][0][2][0][1]\n",
    "                W = tf.constant(W)        # we don't want to update the parameters\n",
    "                b = tf.constant(np.reshape(b, (b.size)))\n",
    "                graph[layer_name] = tf.nn.conv2d(graph[prev], filter=W, strides=[1, 1, 1, 1], padding=\"SAME\") + b\n",
    "            elif layer_type == \"relu\":\n",
    "                graph[layer_name] = tf.nn.relu(graph[prev])\n",
    "            elif layer_type == \"pool\":    # according to the paper, average pooling behaves better\n",
    "                graph[layer_name] = tf.nn.avg_pool(graph[prev], ksize=[1, 2, 2, 1], \n",
    "                                                   strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "            else:\n",
    "                raise Exception(\"Unknown layer\")\n",
    "            \n",
    "            prev = layer_name\n",
    "        return graph, layer_names\n",
    "    \n",
    "    def get_layer_names(self):\n",
    "        return self.layer_names\n",
    "    \n",
    "    def get_instance(model_path, force=False):\n",
    "        raise Exception(\"Do not instantiate this class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two child classes\n",
    "Those two classes specify the parameters such as which layer to use for content loss and which layers are used for style loss. Modify according to your needs. Also, note that those two classes are implemented with singleton support. Using singleton prevents the overhead for loading the weights on everyrun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class VGG19(VGG):\n",
    "    \n",
    "    instance = None\n",
    "    \n",
    "    def __init__(self, model_path):\n",
    "        super(VGG19, self).__init__(model_path)\n",
    "        self.content_layer = \"conv4_2\"\n",
    "        self.style_layer = [\n",
    "            (\"relu1_1\", 0.2),\n",
    "            (\"relu2_1\", 0.2),\n",
    "            (\"relu3_1\", 0.2),\n",
    "            (\"relu4_1\", 0.2),\n",
    "            (\"relu5_1\", 0.2)\n",
    "        ]\n",
    "    \n",
    "    def get_instance(model_path, force=False):\n",
    "        \"\"\"\n",
    "        Singleton to avoid load graph on every run\n",
    "        If force is True, then force reload\n",
    "        \"\"\"\n",
    "        if VGG19.instance == None or force:\n",
    "            VGG19.instance = VGG19(model_path)\n",
    "        return VGG19.instance\n",
    "    \n",
    "    get_instance = staticmethod(get_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class VGG16(VGG):\n",
    "    \n",
    "    instance = None\n",
    "    \n",
    "    def __init__(self, model_path):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.graph = self.load_model()\n",
    "        self.content_layer = \"relu4_2\"\n",
    "        self.style_layer = [\n",
    "            (\"relu1_2\", 0.5),\n",
    "            (\"relu2_2\", 1.0),\n",
    "            (\"relu3_3\", 1.5),\n",
    "            (\"relu4_3\", 1.5),\n",
    "            (\"relu5_3\", 4.0)\n",
    "        ]\n",
    "    \n",
    "    def get_instance(model_path, force=False):\n",
    "        \"\"\"\n",
    "        Singleton to avoid load graph on every run\n",
    "        If force is True, then force reload\n",
    "        \"\"\"\n",
    "        if VGG16.instance == None or force:\n",
    "            VGG16.instance = VGG16(model_path)\n",
    "        return VGG16.instance\n",
    "    \n",
    "    get_instance = staticmethod(get_instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Factory to create the corresponding VGG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VGGFactory(object):\n",
    "    \n",
    "    def factory(name, model_path, force=False):\n",
    "        \"\"\"\n",
    "        The factory to create the corresponding model we will use\n",
    "        Available names include \"VGG16\" and \"VGG19\"\n",
    "        If force is set, then force reload the graph\n",
    "        \"\"\"\n",
    "        if name == \"VGG16\": return VGG16.get_instance(model_path, force)\n",
    "        if name == \"VGG19\": return VGG19.get_instance(model_path, force)\n",
    "    \n",
    "    factory = staticmethod(factory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class for real training\n",
    "The MagicPen class, as its name says, draws intermixed pictures. To run, you need to provide which VGG model you want to use (VGG19 or VGG16), coupled with the path of the model (see the bash script at the top for the location) and the path of the style image. You don't need to provide alpha (the content loss weight) or beta (the style loss weight) during the object instantiation time, but you do need to set them before calling draw()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MagicPen(object):\n",
    "    \"\"\"\n",
    "    The actual class that carries out the image generation\n",
    "    \"\"\"\n",
    "    \n",
    "    # means for RGB channels. I believe it's the training set average for the pretrained model,\n",
    "    # according to this: https://gist.github.com/ksimonyan/211839e770f7b538e2d8. Also various \n",
    "    # sources agree on those values.\n",
    "    IMG_MEAN = np.array([123.68, 116.779, 103.939], dtype=np.float32).reshape((1,1,1,3))\n",
    "    \n",
    "    RANDOM_RATE = 0.4 # The amount of interpolation of random image to the content image when \n",
    "                      # creating the base image\n",
    "    \n",
    "    def __init__(self, vgg_name, vgg_path, style_path, alpha=None, beta=None, iterations=None):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.iterations = iterations\n",
    "        print(\"Load graph\")\n",
    "        self.vgg = VGGFactory.factory(vgg_name, vgg_path)\n",
    "        self.style_path = style_path\n",
    "        self.opt_algo = \"lbfgsb\"\n",
    "        print(\"OK\")\n",
    "        \n",
    "    def _generate_base(self, content):\n",
    "        \"\"\"\n",
    "        Add some noise to the content image to get the base image\n",
    "        \"\"\"\n",
    "        random_img = np.random.uniform(-20, 20, content.shape)\n",
    "        return MagicPen.RANDOM_RATE * random_img + content * (1 - MagicPen.RANDOM_RATE)\n",
    "        \n",
    "    def _process_img(self, image):\n",
    "        \"\"\"\n",
    "        Returns the image with 4 dimensions\n",
    "        \"\"\"\n",
    "        image = np.float32(image)\n",
    "        image = np.reshape(image, ((1,) + image.shape))\n",
    "        image -= MagicPen.IMG_MEAN\n",
    "        return image\n",
    "    \n",
    "    def set_alpha(self, alpha):\n",
    "        self.alpha = alpha\n",
    "        return self\n",
    "    \n",
    "    def set_beta(self, beta):\n",
    "        self.beta = beta\n",
    "        return self\n",
    "    \n",
    "    def set_variance_wieght(self, v_weight):\n",
    "        self.v_weight = v_weight\n",
    "        return self\n",
    "    \n",
    "    def set_iters(self, iterations):\n",
    "        self.iterations = iterations\n",
    "        return self\n",
    "    \n",
    "    def load_content(self, image_path, ratio=1.0):\n",
    "        \"\"\"\n",
    "        Load the content image, with the option to resize the image. \n",
    "        ratio -- the ratio of new size to old size. A float number\n",
    "        \"\"\"\n",
    "        image = PIL.Image.open(image_path)\n",
    "        image = image.resize((int(image.size[0] * ratio), int(image.size[1] * ratio)), PIL.Image.ANTIALIAS)\n",
    "        self.content = self._process_img(image)\n",
    "        self.base = self._generate_base(self.content)\n",
    "        return self\n",
    "    \n",
    "    def gram_matrix(self, F, N, M):\n",
    "        \"\"\"\n",
    "        The gram matrix G.\n",
    "        F -- the features\n",
    "        N -- number of filters\n",
    "        M -- hight x width of one feature map \n",
    "        Names as per paper\n",
    "        \"\"\"\n",
    "        G = tf.reshape(F, (M, N))\n",
    "        return tf.matmul(tf.transpose(G), G)\n",
    "    \n",
    "    def content_loss(self, sess, graph):\n",
    "        \"\"\"\n",
    "        Compute the context loss as described in the paper. We only need to do the forward \n",
    "        pass once on the content image\n",
    "        \n",
    "        sess -- the current session\n",
    "        content_img -- the content image. should be a numpy array with dimension [1, hight, width, 3]\n",
    "        Note: the dimension of the image should match with the one set at resize_input.\n",
    "        Also, the image should be centered. The mean to subtract will be the training set mean of the VGG network.\n",
    "        \"\"\"\n",
    "        graph[\"input\"].assign(self.content).eval()\n",
    "        P = sess.run(graph[self.vgg.content_layer])\n",
    "        F = graph[self.vgg.content_layer]\n",
    "        N = P.shape[3]  # number of filters\n",
    "        M = P.shape[1] * P.shape[2] # hight x width of one feature map \n",
    "        return 0.5 * tf.reduce_sum(tf.pow(F - P, 2))\n",
    "        \n",
    "    \n",
    "    def style_loss(self, sess, graph):\n",
    "        \"\"\"\n",
    "        Compute the style loss as described in the papaer. Again, only do forward pass once for style image\n",
    "        \n",
    "        sess -- the current session\n",
    "        graph - the current graph\n",
    "        \"\"\"\n",
    "        loss = 0.0\n",
    "        style_img = PIL.Image.open(self.style_path) \\\n",
    "                       .resize((self.content.shape[2], self.content.shape[1]), PIL.Image.ANTIALIAS)\n",
    "        style_img = self._process_img(style_img)\n",
    "\n",
    "        graph[\"input\"].assign(style_img).eval()\n",
    "        for key, w in self.vgg.style_layer:\n",
    "            print(key)\n",
    "            P = sess.run(graph[key])\n",
    "            F = graph[key]\n",
    "            N = P.shape[3]  # number of filters\n",
    "            M = P.shape[1] * P.shape[2] # hight x width of one feature map \n",
    "            A = self.gram_matrix(P, N, M)\n",
    "            G = self.gram_matrix(F, N, M)\n",
    "            loss += ((1.0 / (4 * N**2 * M**2)) * tf.reduce_sum(tf.pow(G - A, 2)) * w)\n",
    "            \n",
    "        return loss\n",
    "    \n",
    "    def set_optimize_algo(self, algo):\n",
    "        if algo == \"adam\" or algo == \"lbfgsb\":\n",
    "            self.opt_algo = algo\n",
    "        else:\n",
    "            raise Exception('Invalid optimization algorithm choice. Available options are: \"adam\" and \"lbfgsb\"')\n",
    "        return self\n",
    "    \n",
    "    def total_variation_loss(self, graph):\n",
    "        b, h, w, d = self.content.shape\n",
    "        shape = self.content.shape\n",
    "        x = graph[\"input\"]\n",
    "        tv_y_size = shape[0] * (shape[1] - 1) * shape[2] * shape[3]\n",
    "        tv_x_size = shape[0] * shape[1] * (shape[2] - 1) * shape[3]\n",
    "        loss_y = tf.nn.l2_loss(x[:,1:,:,:] - x[:,:-1,:,:]) \n",
    "        loss_y /= tv_y_size\n",
    "        loss_x = tf.nn.l2_loss(x[:,:,1:,:] - x[:,:,:-1,:]) \n",
    "        loss_x /= tv_x_size\n",
    "        loss = 2 * (loss_y + loss_x)\n",
    "        loss = tf.cast(loss, tf.float32)\n",
    "        return loss\n",
    "    \n",
    "    def draw(self):\n",
    "        \"\"\"\n",
    "        The function to finally generate the image\n",
    "        \"\"\"\n",
    "        graph, _ = self.vgg.create_graph(self.content.shape)\n",
    "        image = None\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            print(\"initialize variables\")\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            print(\"compute content loss\")\n",
    "            content_loss = self.content_loss(sess, graph) * self.alpha\n",
    "            print(\"compute style loss\")\n",
    "            style_loss   = self.style_loss(sess, graph) * self.beta\n",
    "            print(\"compute total loss\")\n",
    "            tv_loss      = self.total_variation_loss(graph) * self.v_weight\n",
    "            print(\"total variational loss\")\n",
    "            total_loss   = content_loss + style_loss + tv_loss\n",
    "            \n",
    "            train_step   = tf.train.AdamOptimizer(2.0).minimize(total_loss)\n",
    "            \n",
    "            print(\"reinitialize the variables\")\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            graph[\"input\"].assign(self.base).eval()\n",
    "            \n",
    "            if self.opt_algo == \"adam\":\n",
    "                ##### Adam optimization algorithm\n",
    "                print(\"optimization using adam starts\")\n",
    "                for i in range(self.iterations):\n",
    "                    sess.run(train_step)\n",
    "                    if (i + 1) % 10 == 0:\n",
    "                        print(\".\", end=\" \")\n",
    "                    if (i + 1) % 100 == 0:\n",
    "                        c_loss = content_loss.eval()\n",
    "                        s_loss = style_loss.eval()\n",
    "                        v_loss = tv_loss.eval()\n",
    "                        t_loss = total_loss.eval()\n",
    "                        print()\n",
    "                        print(\"%f, %f, %f, %f\" % (c_loss, s_loss, v_loss, t_loss))\n",
    "            \n",
    "            if self.opt_algo == \"lbfgsb\":\n",
    "                ##### L-BFGS-B optimization algorithm\n",
    "                print(\"optimization using lbfgsb starts\")\n",
    "            \n",
    "                step = [0]\n",
    "                \n",
    "                def _step(*args):\n",
    "                    \"\"\"\n",
    "                    A hook to count the iteration number\n",
    "                    \"\"\"\n",
    "                    step[0] = step[0] + 1\n",
    "                    if (step[0] + 1) % 10 == 0:\n",
    "                        print(\".\", end=\" \")\n",
    "                \n",
    "                optimizer = tf.contrib.opt.ScipyOptimizerInterface(\n",
    "                            total_loss, method='L-BFGS-B',\n",
    "                            options={'maxiter': self.iterations})\n",
    "                optimizer.minimize(sess, step_callback=_step)\n",
    "            \n",
    "            image = graph[\"input\"].eval()\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        image += MagicPen.IMG_MEAN\n",
    "        \n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to free up the object on multiple runs. Maybe helpful if Tensorflow fails to clean up the GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del pen\n",
    "except NameError:\n",
    "    print(\"pen is not defined\")\n",
    "else:\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example run. The default algorithm is L-BFGS-B. Notice the model path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load graph\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "pen = MagicPen(\"VGG19\", \"models/imagenet-vgg-verydeep-19.mat\", \"pictures/styles/wave.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize variables\n",
      "compute content loss\n",
      "compute style loss\n",
      "relu1_1\n",
      "relu2_1\n",
      "relu3_1\n",
      "relu4_1\n",
      "relu5_1\n",
      "compute total loss\n",
      "total variational loss\n",
      "reinitialize the variables\n",
      "optimization using lbfgsb starts\n"
     ]
    }
   ],
   "source": [
    "image = pen.load_content(\"pictures/scenery.jpg\", ratio=0.8).set_alpha(0.5).set_beta(10000).set_variance_wieght(1e3).set_iters(300).draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An auxiliary function to visualize the image. Same as in the Tensorflow deep dream tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def showarray(a, fmt='jpeg'):\n",
    "    a = np.uint8(np.clip(a, 0, 255))\n",
    "    f = BytesIO()\n",
    "    PIL.Image.fromarray(a).save(f, fmt)\n",
    "    display(Image(data=f.getvalue()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "converted_image = np.clip((image[0]), 0, 255).astype('uint8')\n",
    "showarray(converted_image)\n",
    "print(converted_image.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
